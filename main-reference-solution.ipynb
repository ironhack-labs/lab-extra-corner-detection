{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsTnVufoNjkM"
   },
   "source": [
    "# Lab (extra) | Corner detection and ORB (reference solution)\n",
    "\n",
    "At the end of this laboratory, you would get familiarized with\n",
    "\n",
    "*   Corner Detection using FAST and Harris Corners\n",
    "*   ORB feature detectors and descriptors\n",
    "\n",
    "**Remember this is a graded exercise.**\n",
    "\n",
    "*   For every plot, make sure you provide appropriate titles, axis labels, legends, wherever applicable.\n",
    "*   Create reusable functions where ever possible, so that the code could be reused at different places.\n",
    "*   Mount your drive to access the images.\n",
    "*   Add sufficient comments and explanations wherever necessary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23152,
     "status": "ok",
     "timestamp": 1666024092762,
     "user": {
      "displayName": "Bhalaji Nagarajan",
      "userId": "08128536337584225726"
     },
     "user_tz": -120
    },
    "id": "jJIC9t_DV7js",
    "outputId": "7ae71d07-99ba-4e6d-99db-70d9a071d483"
   },
   "outputs": [],
   "source": [
    "# mount drive to the notebook\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1T3jZ0kI-XX"
   },
   "outputs": [],
   "source": [
    "# Loading necessary libraries (Feel free to add new libraries if you need for any computation)\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from skimage import color, data, feature, filters, io, transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anxMaT2X_9JG"
   },
   "outputs": [],
   "source": [
    "img_fld_path = r'/content/drive/MyDrive/2122_CV_Practicals/P05_02_nov_2021_corner_detection_and_ORB/images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ir6WNbB4L05G"
   },
   "source": [
    "# Corner Detection\n",
    "\n",
    "### Exercise: #1\n",
    "\n",
    "*    Detect the corners in the image starbucks4.jpg. Use corner_fast to extract FAST corners for the given image and corner_peaks to find corners in the corner measure response image.\n",
    "*   Show the original image, fast corners, peaks overlayed on the original image using matplotlib subplot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 1651,
     "status": "ok",
     "timestamp": 1666024095990,
     "user": {
      "displayName": "Bhalaji Nagarajan",
      "userId": "08128536337584225726"
     },
     "user_tz": -120
    },
    "id": "iun4uPF7MVIj",
    "outputId": "5c0a00ec-1811-4b96-b098-06c7d03f5c06"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "img = io.imread(os.path.join(img_fld_path, 'starbucks/starbucks4.jpg'))\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 1700,
     "status": "ok",
     "timestamp": 1666024097679,
     "user": {
      "displayName": "Bhalaji Nagarajan",
      "userId": "08128536337584225726"
     },
     "user_tz": -120
    },
    "id": "Fb7nZ6NyMbwT",
    "outputId": "f13cbb16-9738-49e5-e39e-26b5e14d9a55"
   },
   "outputs": [],
   "source": [
    "img = color.rgb2gray(img)\n",
    "\n",
    "fast= feature.corner_fast(img)\n",
    "plt.imshow(fast)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1666024097679,
     "user": {
      "displayName": "Bhalaji Nagarajan",
      "userId": "08128536337584225726"
     },
     "user_tz": -120
    },
    "id": "CznlkH1KMibG",
    "outputId": "d0e4465e-ce75-4c5c-f655-3d2d3831cc2b"
   },
   "outputs": [],
   "source": [
    "corner_points = feature.corner_peaks(fast)\n",
    "plt.imshow(img, cmap=plt.cm.gray)\n",
    "plt.scatter(corner_points[:, 1], corner_points[:, 0], facecolors='none', edgecolors='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlR3AhnMMV9n"
   },
   "source": [
    "*   Analyze and discuss the effect of changing **'threshold'** in the corner_fast function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmXPb6JMMcKb"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "The higher the threshold value is then fewer local features are detected. According to the scikit documentation:\n",
    "\n",
    "Threshold used in deciding whether the pixels on the circle are brighter, darker or similar w.r.t. the test pixel. Decrease the threshold when more corners are desired and vice-versa.\n",
    "\n",
    "We consider that the threshold of 0.2 is pretty accurate. Values under 0.2 detect some edges as corners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfOF0CeJNMMD"
   },
   "source": [
    "*   Repeat the exercise by replacing FAST corners with Harris corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1666024097680,
     "user": {
      "displayName": "Bhalaji Nagarajan",
      "userId": "08128536337584225726"
     },
     "user_tz": -120
    },
    "id": "fJcx3ZZFNT48",
    "outputId": "332054a4-a9cd-49a8-bd66-21f50248de42"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "img = color.rgb2gray(img)\n",
    "\n",
    "harris_corners= feature.corner_harris(img)\n",
    "\n",
    "corner_points = feature.corner_peaks(harris_corners)\n",
    "plt.imshow(img, cmap=plt.cm.gray)\n",
    "plt.scatter(corner_points[:, 1], corner_points[:, 0], facecolors='none', edgecolors='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSqjxrhINVur"
   },
   "source": [
    "*   Do you find difference between both the techniques? If so, why and what difference do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8nfKNaANh7R"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjTXqXXX4_Lj"
   },
   "source": [
    "# ORB feature detector and binary descriptor\n",
    "\n",
    "### Exercise #0: Compute ORB descriptors and find descriptors match\n",
    "\n",
    "*   Load 'astronaut' image from data module.\n",
    "*   Convert the image to grayscale.\n",
    "*   Create a copy of the image and rotate it by 180&deg; \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh61dfwg_NgJ"
   },
   "outputs": [],
   "source": [
    "img1 = color.rgb2gray(data.astronaut())\n",
    "img2 = transform.rotate(img1, 180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0jcF2EbDRnp"
   },
   "source": [
    "*   Create an ORB feature detector with default parameters. *Hint: ORB feature detector is available in skimage.feature module.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hI3dVPnj_xj_"
   },
   "outputs": [],
   "source": [
    "descriptor_extractor = feature.ORB(n_keypoints=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvU6NlYCDcdY"
   },
   "source": [
    "*   Extract the keypoints and descriptors using detect_and_extract function for both the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlCJtotkDi7j"
   },
   "outputs": [],
   "source": [
    "descriptor_extractor.detect_and_extract(img1)\n",
    "keypoints1 = descriptor_extractor.keypoints\n",
    "descriptors1 = descriptor_extractor.descriptors\n",
    "\n",
    "descriptor_extractor.detect_and_extract(img2)\n",
    "keypoints2 = descriptor_extractor.keypoints\n",
    "descriptors2 = descriptor_extractor.descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRR6cdGEEFJJ"
   },
   "source": [
    "*   Descriptors can be matched using match_descriptors function available in skimage.feature module\n",
    "*   The matches can be plotted using the plot_matches function available in skimage.feature module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "executionInfo": {
     "elapsed": 1120,
     "status": "ok",
     "timestamp": 1666024099912,
     "user": {
      "displayName": "Bhalaji Nagarajan",
      "userId": "08128536337584225726"
     },
     "user_tz": -120
    },
    "id": "4O2sTS4h73jz",
    "outputId": "1cb41eff-f0b0-4aaf-eb30-190700cadff7"
   },
   "outputs": [],
   "source": [
    "matches12 = feature.match_descriptors(descriptors1, descriptors2, cross_check=True)\n",
    "feature.plot_matches(plt, img1, img2, keypoints1, keypoints2, matches12)\n",
    "plt.axis('off')\n",
    "plt.title(\"Original Image vs. Transformed Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSaDWEpfFrge"
   },
   "source": [
    "### Exercise: #1\n",
    "\n",
    "*   Analyze and discuss the effect of changing the parameter **'max_ratio'** in the match_descriptors function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqTGqWE-F0Z7"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJQV1TIeIgGf"
   },
   "source": [
    "*   What do you infer from **'harris_k'**, **'n_scales'**, **'fast_n'**, **'fast_threshold'** parameters in ORB function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1qUG2FuJCUi"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qewFLt7sEs93"
   },
   "source": [
    "### Exercise: #2\n",
    "*   Using the above example, create a function get_ORB, that takes two images as parameters and returns the keypoints of both images and descriptor matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3H6M75aEsaC"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "#Function to obtain the keypoints of both images (according to Harris corner algortihm response) and the match descriptor\n",
    "#matrix (Indices of corresponding matches in first and second set of descriptors, where matches[:, 0] denote the indices in \n",
    "# the first and matches[:, 1] the indices in the second set of descriptors.)\n",
    "def get_ORB(img1,img2):\n",
    "    img1 = color.rgb2gray(img1)\n",
    "    img2 = color.rgb2gray(img2)\n",
    "    \n",
    "    descriptor_extractor = feature.ORB(n_keypoints=200, fast_threshold=0.5)\n",
    "    #fast_threshold=0.3, harris_k=0.04)\n",
    "\n",
    "    #Obtaining the keypoints and the descriptor vector from the first image\n",
    "    descriptor_extractor.detect_and_extract(img1)\n",
    "    keypoints1 = descriptor_extractor.keypoints\n",
    "    descriptors1 = descriptor_extractor.descriptors\n",
    "\n",
    "    #Obtaining the keypoints and the descriptor vector from the second image\n",
    "    descriptor_extractor.detect_and_extract(img2)\n",
    "    keypoints2 = descriptor_extractor.keypoints\n",
    "    descriptors2 = descriptor_extractor.descriptors\n",
    "    \n",
    "    #Generating the match descriptor matrix\n",
    "    matches12 = feature.match_descriptors(descriptors1, descriptors2, cross_check=True)\n",
    "    \n",
    "    return matches12,keypoints1,keypoints2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee3MqFlgFfUP"
   },
   "source": [
    "*   With the function, detect the correspondences between the model image starbucks.jpg with the scene image starbucks4.jpg\n",
    "*   Plot the matches between the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "executionInfo": {
     "elapsed": 821,
     "status": "ok",
     "timestamp": 1666024100728,
     "user": {
      "displayName": "Bhalaji Nagarajan",
      "userId": "08128536337584225726"
     },
     "user_tz": -120
    },
    "id": "OfuTiwfxFhRf",
    "outputId": "da9ab71a-5159-4c43-e7ad-38e7aa6069b1"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "img1 = io.imread(os.path.join(img_fld_path, 'starbucks/starbucks.jpg'))\n",
    "img2 = io.imread(os.path.join(img_fld_path, 'starbucks/starbucks4.jpg'))\n",
    "\n",
    "#Plotting the ORB result\n",
    "matches,keypoints1,keypoints2 = get_ORB(img1,img2)\n",
    "f,axarray = plt.subplots()\n",
    "feature.plot_matches(axarray, img1,img2,keypoints1,keypoints2,matches)\n",
    "axarray.set_title('ORB Result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7iNt-h8GIub"
   },
   "source": [
    "### Exercise: #3\n",
    "\n",
    "*   Using \"starbucks.jpg\" image as a model, show its matches to all Starbucks images. \n",
    "*   Show the number of the matches for the pair as title for each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 5955,
     "status": "ok",
     "timestamp": 1666024106678,
     "user": {
      "displayName": "Bhalaji Nagarajan",
      "userId": "08128536337584225726"
     },
     "user_tz": -120
    },
    "id": "Gpz290LqGcDk",
    "outputId": "e79e6868-1497-4b28-ee83-41d5636ddde0"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "starbucks_model = io.imread(os.path.join(img_fld_path, 'starbucks/starbucks.jpg'))\n",
    "starbucks = []\n",
    "starbucks.append(io.imread(os.path.join(img_fld_path, 'starbucks/starbucks2.png')))\n",
    "starbucks.append(io.imread(os.path.join(img_fld_path, 'starbucks/starbucks4.jpg')))\n",
    "starbucks.append(io.imread(os.path.join(img_fld_path, 'starbucks/starbucks5.png')))\n",
    "starbucks.append(io.imread(os.path.join(img_fld_path, 'starbucks/starbucks6.jpg')))\n",
    "starbucks.append(io.imread(os.path.join(img_fld_path, 'starbucks/starbucksCup.jpg')))\n",
    "\n",
    "#Obtaining the different matches and keypoints from the comparison between the model image and the\n",
    "#rest of them\n",
    "matchm = []\n",
    "keypoints = []\n",
    "for i in range(5):\n",
    "    m,keypointsm,k = get_ORB(starbucks_model,starbucks[i])\n",
    "    matchm.append(m)\n",
    "    keypoints.append(k)\n",
    "    \n",
    "#Plotting them\n",
    "for i in range(5):\n",
    "    f,axarray = plt.subplots()\n",
    "    feature.plot_matches(axarray, color.rgb2gray(starbucks_model),color.rgb2gray(starbucks[i]),keypointsm,keypoints[i],matchm[i])\n",
    "    axarray.set_title('ORB Match Result ' + str(len(matchm[i])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXTgi_zeGeiS"
   },
   "source": [
    "*   Comment on the performance of the algorithm with respect to each image. When do you think the algorithm works better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79m3V4oiGpJM"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cY0FdG0G-Oq"
   },
   "source": [
    "### Exercise: #4\n",
    "\n",
    "*   Repeate **Exercise #3**, by changing the orientation of the model image by rotating it and comparing it with its original version.\n",
    "*   Create 8 orientations of the model image by rotating the image by 45&deg; (0, 45, 90, 135, 180, 225, 270, 315). *Hint: You can use the rotate() function from skimage.transform.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6669,
     "status": "ok",
     "timestamp": 1666024113330,
     "user": {
      "displayName": "Bhalaji Nagarajan",
      "userId": "08128536337584225726"
     },
     "user_tz": -120
    },
    "id": "iAdLWbkGHysK",
    "outputId": "047e66c0-2a34-45ec-c9b1-5eaa2deb4324"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "img = io.imread(os.path.join(img_fld_path, 'starbucks/starbucks.jpg'))\n",
    "\n",
    "orientations = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "\n",
    "for orientation in orientations:\n",
    "    img2 = transform.rotate(img, orientation, resize=False)\n",
    "    #Plotting the ORB result\n",
    "    matches,keypoints1,keypoints2 = get_ORB(img,img2)\n",
    "    f,axarray = plt.subplots()\n",
    "    feature.plot_matches(axarray, img,img2,keypoints1,keypoints2,matches)\n",
    "    axarray.set_title('ORB Result')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpQtm37M2AD5"
   },
   "source": [
    "*   What do you observe with respect to the change in orientations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgMbkR_g18nP"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwb8qZz3IGCo"
   },
   "source": [
    "### Exercise: #5\n",
    "\n",
    "*   Repeat **Exercise #3** using Coco-cola images using cocacola_logo.png as the model image.\n",
    "*   Does the same parameters used for Starbucks images work in this case? Or was it necessary to modify the ORB parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 16645,
     "status": "ok",
     "timestamp": 1666024129972,
     "user": {
      "displayName": "Bhalaji Nagarajan",
      "userId": "08128536337584225726"
     },
     "user_tz": -120
    },
    "id": "YEUq65_fIC3u",
    "outputId": "c4383580-ab94-4b87-ae8c-3f748b888f5e"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "cocacola_model = io.imread(os.path.join(img_fld_path, 'cocacola/cocacola_logo.png'))\n",
    "cocacola = []\n",
    "cocacola.append(io.imread(os.path.join(img_fld_path, 'cocacola/cocacola1.jpg')))\n",
    "cocacola.append(io.imread(os.path.join(img_fld_path, 'cocacola/cocacola2.png')))\n",
    "cocacola.append(io.imread(os.path.join(img_fld_path, 'cocacola/cocacola3.jpg')))\n",
    "cocacola.append(io.imread(os.path.join(img_fld_path, 'cocacola/cocacola4.jpg')))\n",
    "cocacola.append(io.imread(os.path.join(img_fld_path, 'cocacola/cocacola5.png')))\n",
    "cocacola.append(io.imread(os.path.join(img_fld_path, 'cocacola/cocacola6.jpg')))\n",
    "\n",
    "#Obtaining the different matches and keypoints from the comparison between the model image and the\n",
    "#rest of them\n",
    "matchm = []\n",
    "keypoints = []\n",
    "for i in range(5):\n",
    "    m,keypointsm,k = get_ORB(cocacola_model,cocacola[i])\n",
    "    matchm.append(m)\n",
    "    keypoints.append(k)\n",
    "    \n",
    "#Plotting them\n",
    "for i in range(5):\n",
    "    f,axarray = plt.subplots()\n",
    "    feature.plot_matches(axarray, color.rgb2gray(cocacola_model),color.rgb2gray(cocacola[i]),keypointsm,keypoints[i],matchm[i])\n",
    "    axarray.set_title('ORB Match Result ' + str(len(matchm[i])))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83tMJ67a15G-"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4C5K2SzByRL"
   },
   "source": [
    "### Exercise #6. Analysis of the applied techniques and results\n",
    "\n",
    "*   What are the advantages of ORB object detection compared with HOG object detector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ret9l0NHimS1"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVjZhZOSCA4u"
   },
   "source": [
    "*   What would happen if you analyse an image that does not contain the Starbucks logo (as is the case of \"edificio.jpg\")?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "executionInfo": {
     "elapsed": 2034,
     "status": "ok",
     "timestamp": 1666024132004,
     "user": {
      "displayName": "Bhalaji Nagarajan",
      "userId": "08128536337584225726"
     },
     "user_tz": -120
    },
    "id": "URIfq2NbCMRL",
    "outputId": "8b497a36-467a-43e2-ae9f-fe77271252dd"
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "img1 = io.imread(os.path.join(img_fld_path, 'starbucks/starbucks.jpg'))\n",
    "img2 = io.imread(os.path.join(img_fld_path, 'edificio.jpg'))\n",
    "\n",
    "#Plotting the ORB result\n",
    "matches,keypoints1,keypoints2 = get_ORB(img1,img2)\n",
    "f,axarray = plt.subplots()\n",
    "feature.plot_matches(axarray, img1,img2,keypoints1,keypoints2,matches)\n",
    "axarray.set_title('ORB Result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt5XSvpU12jP"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gvg-n9QJCM9f"
   },
   "source": [
    "*   What are the different ways of defining a quality measure for the correspondance between two images? (implementation of measures is not necessary)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5M5MGDUCPqr"
   },
   "source": [
    "**Solution**\n",
    "\n",
    "*(Double-click or enter to edit)*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2odhSVaJ44I"
   },
   "source": [
    "# Panorama Stitching (Not graded)\n",
    "\n",
    "Image Mosaicing is a technique of combining multiple overlapping images into a single image.\n",
    "\n",
    "*   A simple tutorial can be found in this notebook: \n",
    "*   https://github.com/scikit-image/skimage-tutorials/blob/main/lectures/solutions/adv3_panorama-stitching-solution.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeSGQcZz/5OvYfIJdWiwOU",
   "collapsed_sections": [],
   "mount_file_id": "1jKrOONLYPmArVSf5AKIJFtxMRIy7lQXb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
